# LLM Monitoring & Evaluation Pipeline (MLOps)

A **production-grade LLM monitoring and evaluation system** built to detect hallucinations, relevance degradation, and reliability issues in Large Language Model (LLM outputs).
The system exposes a FastAPI service for LLM inference, evaluates responses using embedding-based metrics and heuristics, and tracks experiments using MLflow to enable regression analysis across model and prompt iterations.

---

## ğŸš€ Key Features

* **LLM Inference API**

  * FastAPI-based REST service
  * Pluggable LLM backend (open-source Hugging Face models)
  * Schema-validated request and response contracts

* **Automated LLM Evaluation**

  * Embedding-based relevance scoring (cosine similarity)
  * Heuristic-based hallucination detection
  * Structured evaluation results returned with each response

* **MLOps & Monitoring**

  * MLflow experiment tracking (file-based, no external infrastructure)
  * Logging of prompts, models, metrics, and raw outputs
  * Regression analysis across prompt and model iterations

* **Production-Oriented Design**

  * Modular architecture
  * Clear separation of concerns
  * Easily extensible for new metrics, models, or evaluators

---

## ğŸ§  Motivation

In real-world GenAI systems, **LLM outputs cannot be blindly trusted**.
This project focuses on **detecting failures**, not hiding them.

Instead of optimizing for â€œperfect answersâ€, it:

* Surfaces hallucinations
* Measures semantic relevance
* Tracks quality drift over time

This mirrors how **production GenAI and MLOps teams** monitor LLM reliability.

---

## ğŸ—ï¸ Architecture Overview

```
Client (Swagger / API)
        |
        v
FastAPI (/generate)
        |
        v
LLM Client (Local Hugging Face Model)
        |
        v
Raw LLM Output
        |
        v
Evaluation Pipeline
   â”œâ”€â”€ Relevance Scoring (Embeddings)
   â”œâ”€â”€ Hallucination Detection (Heuristics)
        |
        v
EvaluationResult
        |
        v
MLflow Tracking (Metrics + Artifacts)
```

---

## ğŸ“‚ Project Structure

```
llm-monitoring-evaluation-pipeline/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py            # FastAPI endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py            # Configuration handling
â”‚   â”‚   â””â”€â”€ logging.py           # Logging setup
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ client.py            # LLM backend abstraction
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ request.py           # Request schemas
â”‚   â”‚   â””â”€â”€ response.py          # Response schemas
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ evaluator.py         # Evaluation orchestrator
â”‚   â”‚   â”œâ”€â”€ relevance.py         # Relevance scoring
â”‚   â”‚   â”œâ”€â”€ hallucination.py     # Hallucination detection
â”‚   â”‚   â””â”€â”€ result.py            # Evaluation result schema
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â””â”€â”€ mlflow_tracker.py    # MLflow logging
â”‚   â””â”€â”€ main.py                  # Application entrypoint
â”‚
â”œâ”€â”€ mlruns/                       # MLflow runs (local)
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

---

## ğŸ¤– LLM Used

* **Model:** `distilgpt2`
* **Framework:** Hugging Face `transformers`
* **Inference:** Local CPU (no API keys, no paid services)

### Why `distilgpt2`?

* Small and fast
* Not instruction-tuned
* High hallucination tendency

This makes it **ideal for testing monitoring and evaluation logic**, rather than hiding failures with a strong model.

---

## ğŸ“Š Evaluation Metrics

### 1ï¸âƒ£ Relevance Score

* Computed using sentence embeddings
* Cosine similarity between prompt and generated output
* Range: `[0, 1]`
* Lower score indicates semantic drift or irrelevance

### 2ï¸âƒ£ Hallucination Detection

Heuristic-based detection including:

* Prompt echoing
* Low relevance score
* Suspicious metadata hallucinations (e.g., fabricated articles, dates)

### 3ï¸âƒ£ Schema Validity

* Ensures responses remain structurally valid and API-safe

---

## ğŸ“ˆ MLflow Experiment Tracking

Each `/generate` request logs:

* **Parameters**

  * Prompt
  * Model name
* **Metrics**

  * Relevance score
  * Hallucination flag
* **Artifacts**

  * Raw LLM output

This enables:

* Prompt regression testing
* Model comparison
* Drift and failure trend analysis

---

## ğŸ”Œ API Endpoints

### Health Check

```
GET /health
```

### Generate & Evaluate

```
POST /generate
```

#### Request

```json
{
  "prompt": "Explain LLM hallucination in one sentence."
}
```

#### Response

```json
{
  "raw_output": "...",
  "evaluation": {
    "schema_valid": true,
    "relevance_score": 0.61,
    "hallucination_flag": true,
    "notes": "auto-evaluated"
  }
}
```

Swagger UI is available at:

```
/docs
```

---

## â–¶ï¸ Running the Project

### 1ï¸âƒ£ Create and activate virtual environment

```bash
python -m venv .venv
source .venv/bin/activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Start FastAPI server

```bash
python -m uvicorn app.main:app --reload
```

### 4ï¸âƒ£ Start MLflow UI

```bash
mlflow ui --backend-store-uri ./mlruns
```

---

## ğŸ” Model & Prompt Regression Testing

Because the LLM client is abstracted:

* Models can be swapped with minimal changes
* The same prompts can be re-evaluated
* MLflow metrics enable detection of quality regressions

This mirrors **real-world LLM validation workflows**.

---

## ğŸ“Œ Future Enhancements

* JSON-only LLM outputs with strict schema enforcement
* LLM-as-a-judge hallucination scoring
* Drift alerts and dashboards
* Dockerized deployment
* CI-based evaluation regression tests


